# üöÄ Setup su Nuova Macchina - MediFlow

Questa guida ti mostra come configurare MediFlow su una nuova macchina in pochi minuti.

## üìã Prerequisiti

1. **Docker** installato
2. **Docker Compose** installato
3. **4GB di RAM** libera
4. **5GB di spazio disco** libero
5. **Connessione internet** (solo per il primo avvio)

## ‚ö° Setup Rapido

### 1. Clona il repository
```bash
git clone <repository-url>
cd ai_health_assistant
```

### 2. Avvia tutto con un comando
```bash
docker-compose up -d
```

### 3. Aspetta il completamento
Il sistema impiegher√† circa **5-10 minuti** per:
- Scaricare le immagini Docker
- Avviare il database PostgreSQL
- Scaricare il modello AI llama3.2 (2GB)
- Configurare tutti i servizi

### 4. Verifica il setup
```bash
python test_setup.py
```

### 5. Accedi all'applicazione
Apri il browser e vai su: **http://localhost**

## üéØ Cosa succede automaticamente

1. **Database**: Si avvia con dati di test preconfigurati
2. **Backend**: API FastAPI per gestire l'applicazione
3. **Ollama**: Scarica automaticamente il modello llama3.2
4. **Frontend**: Interfaccia React moderna e responsive

## üë• Credenziali di Accesso

### Paziente
- Email: `patient@test.com`
- Password: `password123`

### Medico
- Email: `doctor@test.com`
- Password: `password123`

### Admin
- Email: `admin@test.com`
- Password: `admin123`

## üîç Verifica del Setup

### Controlla lo stato dei container
```bash
docker-compose ps
```

Dovresti vedere tutti i container con status "Up" e Ollama con "healthy".

### Controlla i log
```bash
# Tutti i servizi
docker-compose logs

# Solo Ollama
docker-compose logs ollama

# Solo frontend
docker-compose logs frontend
```

### Testa la funzionalit√† AI
```bash
# Verifica che Ollama risponda
curl http://localhost:11434/api/tags

# Testa una conversazione
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{"model": "llama3.2", "prompt": "Ciao!", "stream": false}'
```

## üõ†Ô∏è Comandi Utili

```bash
# Ferma tutto
docker-compose down

# Riavvia tutto
docker-compose restart

# Ricostruisci (se necessario)
docker-compose up -d --build

# Visualizza uso risorse
docker stats

# Pulisci tutto (ATTENZIONE: cancella tutti i dati)
docker-compose down -v
docker system prune -a
```

## Endpoint Disponibili

- **Frontend**: http://localhost
- **Backend API**: http://localhost:8001
- **Ollama API**: http://localhost:11434
- **Database**: localhost:5433

## Risoluzione Problemi

### Ollama non scarica il modello
```bash
# Entra nel container
docker exec -it ollama bash

# Scarica manualmente
ollama pull llama3.2

# Verifica i modelli
ollama list
```

### Frontend non carica
```bash
# Verifica che tutti i servizi siano pronti
docker-compose ps

# Ricostruisci il frontend
docker-compose up -d --build frontend
```

### Database non si connette
```bash
# Riavvia il database
docker-compose restart database

# Verifica i log
docker-compose logs database
```

## üìä Monitoraggio

### Stato dei servizi
```bash
docker-compose ps
```

### Log in tempo reale
```bash
docker-compose logs -f
```

### Uso delle risorse
```bash
docker stats
```

## üéâ Successo!

Se tutto funziona correttamente:

1. ‚úÖ Tutti i container sono "Up"
2. ‚úÖ Ollama √® "healthy"
3. ‚úÖ Il test setup passa tutti i controlli
4. ‚úÖ L'applicazione √® accessibile su http://localhost

**Congratulazioni!** Il tuo assistente sanitario AI √® pronto per l'uso!

---

**MediFlow** - Assistente Sanitario Intelligente
